// COMPLEJIDADES TEMPORALES ordenadas de menos eficiente a más eficiente en términos de tiempo de ejecución a medidad que el tamaño de la entrada aumenta:


// 1. **Complejidad Exponencial (O(2^n))**: En estos algoritmos, el tiempo de ejecución aumenta exponencialmente con el tamaño de la entrada. Esto significa que estos algoritmos son muy ineficientes y prácticamente inutilizables para tamaños de entrada moderados o grandes. Ejemplos de algoritmos con complejidad exponencial incluyen la solución por fuerza bruta a problemas de combinaciones o permutaciones.

// 2. **Complejidad Factorial (O(n!))**: Esta es una de las complejidades más ineficientes. En estos algoritmos, el tiempo de ejecución aumenta en función del factorial del tamaño de la entrada. Son extremadamente lentos incluso para entradas pequeñas y generalmente impracticables en la mayoría de los casos. Ejemplos de algoritmos con complejidad factorial incluyen la solución por fuerza bruta a problemas de permutaciones.

// 3. **Complejidad Cúbica (O(n^3))**: En estos algoritmos, el tiempo de ejecución aumenta al cubo con el tamaño de la entrada. Aunque son más eficientes que los dos anteriores, siguen siendo lentos y generalmente poco prácticos para entradas moderadas o grandes. Ejemplos de algoritmos con complejidad cúbica incluyen algunos algoritmos de multiplicación de matrices.

// 4. **Complejidad Cuadrática (O(n^2))**: En estos algoritmos, el tiempo de ejecución aumenta cuadráticamente con el tamaño de la entrada. A menudo son utilizables para tamaños de entrada pequeños o moderados, pero pueden volverse lentos para entradas más grandes. Ejemplos de algoritmos con complejidad cuadrática incluyen Bubble Sort y Selection Sort.

// 5. **Complejidad Lineal (O(n))**: En estos algoritmos, el tiempo de ejecución aumenta de manera lineal con el tamaño de la entrada. Son eficientes y adecuados para tamaños de entrada moderados y grandes. Ejemplos de algoritmos con complejidad lineal incluyen recorrer una lista o suma de elementos en un arreglo.

// 6. **Complejidad Logarítmica (O(log n))**: En estos algoritmos, el tiempo de ejecución crece de forma logarítmica en función del tamaño de la entrada. Son aún más eficientes y adecuados para tamaños de entrada grandes. Ejemplos de algoritmos con complejidad logarítmica incluyen búsqueda binaria en árboles o en arreglos ordenados.

// 7. **Complejidad Logarítmica Lineal (O(n log n))**: Esta es una complejidad común en algoritmos de ordenamiento eficientes, como Quick Sort y Merge Sort. Son muy adecuados para tamaños de entrada grandes y se utilizan ampliamente en aplicaciones del mundo real.

// 8. **Complejidad Lineal Logarítmica (O(n log* n))**: Esta complejidad está asociada con algoritmos altamente eficientes, como el algoritmo de Tarjan para conjuntos disjuntos. Log* es una función logarítmica iterada, lo que significa que es extremadamente eficiente.

// 9. **Complejidad Constante (O(1))**: En estos algoritmos, el tiempo de ejecución es independiente del tamaño de la entrada. Son los algoritmos más eficientes posibles y realizan una cantidad constante de operaciones. Ejemplos de algoritmos con complejidad constante incluyen el acceso a un elemento en un arreglo por índice o el acceso a un elemento en un diccionario.

// La elección del algoritmo adecuado depende de la naturaleza del problema y el tamaño de la entrada. En la mayoría de los casos, se busca utilizar algoritmos con complejidades temporales más eficientes para garantizar un rendimiento óptimo.